{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2022-09-06T06:06:49.745707Z","iopub.execute_input":"2022-09-06T06:06:49.746646Z","iopub.status.idle":"2022-09-06T06:07:01.738521Z","shell.execute_reply.started":"2022-09-06T06:06:49.746558Z","shell.execute_reply":"2022-09-06T06:07:01.737319Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport warnings\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torchmetrics\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom pytorch_lightning import Trainer\nimport openpyxl\nwarnings.filterwarnings(action='ignore',category=UserWarning)\nwarnings.filterwarnings(action='ignore',category=FutureWarning)\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","metadata":{"id":"mSSyiM7OlMWM","outputId":"5bc8a3e5-e075-4593-b565-11aa6638d475","execution":{"iopub.status.busy":"2022-09-06T06:07:01.742156Z","iopub.execute_input":"2022-09-06T06:07:01.742591Z","iopub.status.idle":"2022-09-06T06:07:12.670065Z","shell.execute_reply.started":"2022-09-06T06:07:01.742555Z","shell.execute_reply":"2022-09-06T06:07:12.668845Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"glove_path = '../input/ariticle-bias-smdm-project/glove.6B.100d.txt'","metadata":{"id":"uXIWsIbdlMWQ","execution":{"iopub.status.busy":"2022-09-06T06:07:12.671712Z","iopub.execute_input":"2022-09-06T06:07:12.673523Z","iopub.status.idle":"2022-09-06T06:07:12.679379Z","shell.execute_reply.started":"2022-09-06T06:07:12.673478Z","shell.execute_reply":"2022-09-06T06:07:12.678165Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def Embedding(text,path,max_length,embedding_dim):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(text)\n    word_index=tokenizer.word_index\n    print(\"number of word in vocabulary\",len(word_index))\n    vocab_size = 5000\n    trunc_type = 'post'\n    oov_tok = '<OOV>'\n    padding_type = 'post'\n    #print(\"words in vocab\",word_index)\n    text_sequence=tokenizer.texts_to_sequences(text)\n    text_sequence = pad_sequences(text_sequence, maxlen=max_length, truncating=trunc_type)\n    print(\"word in sentences are replaced with word ID\",text_sequence)\n    size_of_vocabulary=len(tokenizer.word_index) + 1\n    print(\"The size of vocabulary \",size_of_vocabulary)\n    embeddings_index = dict()\n\n    f = open(path)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n    f.close()\n    print('Loaded %s word vectors.' % len(embeddings_index))\n\n    embedding_matrix = np.zeros((size_of_vocabulary, embedding_dim))\n\n    for word, i in tokenizer.word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\n    text_shape = text_sequence.shape\n    X_train = np.empty((text_shape[0],text_shape[1],embedding_matrix.shape[1]))\n    for i in range(text_sequence.shape[0]):\n        for j in range(text_sequence.shape[1]):\n            X_train[i,j,:] = embedding_matrix[text_sequence[i][j]]\n    print(X_train.shape)\n\n    return X_train\n\n\n\ndef Sentence2Vec(filename,glovepath,embedding_dim = 100,max_length = 120):\n    df_train = pd.read_excel(filename,engine='openpyxl')\n    T1 = df_train['content_original'].str.split(' \\n\\n---\\n\\n').str[0]\n    df_train['content_original'] = T1.str.replace('-',' ').str.replace('[^\\w\\s]','').str.replace('\\n',' ').str.lower()\n    df = df_train[['content_original','source','title','bias_text','bias']]\n    \n    S = df['title']\n    T = df['content_original']\n    y_train = df['bias'].to_numpy()\n\n    \n    stop = stopwords.words('english')\n    T = T.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n    S = S.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n    \n    path = glovepath\n\n    head = Embedding(S,path,max_length,embedding_dim)\n    body = Embedding(T,path,max_length,embedding_dim)\n    \n\n    return head,body,y_train","metadata":{"id":"ApZ3rdk7lMWS","execution":{"iopub.status.busy":"2022-09-06T06:07:12.682387Z","iopub.execute_input":"2022-09-06T06:07:12.682870Z","iopub.status.idle":"2022-09-06T06:07:12.699790Z","shell.execute_reply.started":"2022-09-06T06:07:12.682833Z","shell.execute_reply":"2022-09-06T06:07:12.698672Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_head_train,X_body_train,y_train = Sentence2Vec(filename='../input/ariticle-bias-smdm-project/Train.xlsx',glovepath=glove_path, embedding_dim=100, max_length=120)","metadata":{"id":"Ij3a5De3lMWT","outputId":"8535dd13-ae64-4d12-b1bf-a600bd3e557e","execution":{"iopub.status.busy":"2022-09-06T06:07:12.702165Z","iopub.execute_input":"2022-09-06T06:07:12.703107Z","iopub.status.idle":"2022-09-06T06:09:15.739169Z","shell.execute_reply.started":"2022-09-06T06:07:12.703069Z","shell.execute_reply":"2022-09-06T06:09:15.738137Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"idx = np.random.randint(0,X_head_train.shape[0], size = int(0.15*X_head_train.shape[0]), dtype=int)\nidx","metadata":{"execution":{"iopub.status.busy":"2022-09-06T06:09:15.743562Z","iopub.execute_input":"2022-09-06T06:09:15.743842Z","iopub.status.idle":"2022-09-06T06:09:15.753468Z","shell.execute_reply.started":"2022-09-06T06:09:15.743815Z","shell.execute_reply":"2022-09-06T06:09:15.752316Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"idx1 = np.array([i for i in range(X_head_train.shape[0]) if i not in idx])\nidx1","metadata":{"execution":{"iopub.status.busy":"2022-09-06T06:09:15.754977Z","iopub.execute_input":"2022-09-06T06:09:15.755658Z","iopub.status.idle":"2022-09-06T06:09:15.884942Z","shell.execute_reply.started":"2022-09-06T06:09:15.755623Z","shell.execute_reply":"2022-09-06T06:09:15.883920Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_head_val = X_head_train[idx]\nX_body_val = X_body_train[idx]\ny_val = y_train[idx]\n\nX_head_train = X_head_train[idx1]\nX_body_train = X_body_train[idx1]\ny_train = y_train[idx1]","metadata":{"execution":{"iopub.status.busy":"2022-09-06T06:09:15.886236Z","iopub.execute_input":"2022-09-06T06:09:15.886586Z","iopub.status.idle":"2022-09-06T06:09:17.928595Z","shell.execute_reply.started":"2022-09-06T06:09:15.886551Z","shell.execute_reply":"2022-09-06T06:09:17.927526Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(X_head_train.shape)\nprint(X_body_train.shape)\nprint(y_train.shape)\nprint(X_head_val.shape)\nprint(X_body_val.shape)\nprint(y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T06:09:17.930317Z","iopub.execute_input":"2022-09-06T06:09:17.930997Z","iopub.status.idle":"2022-09-06T06:09:17.938957Z","shell.execute_reply.started":"2022-09-06T06:09:17.930960Z","shell.execute_reply":"2022-09-06T06:09:17.937875Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#X_head_val,X_body_val,y_val = Sentence2Vec(filename='../input/ariticle-bias-smdm-project/Test.xlsx',glovepath=glove_path,embedding_dim=100, max_length=120)","metadata":{"id":"lV17_Yr4lMWU","outputId":"782d1116-9e7c-4416-8c53-5dce61088ef8","execution":{"iopub.status.busy":"2022-09-06T06:09:17.949106Z","iopub.execute_input":"2022-09-06T06:09:17.949754Z","iopub.status.idle":"2022-09-06T06:09:17.957601Z","shell.execute_reply.started":"2022-09-06T06:09:17.949716Z","shell.execute_reply":"2022-09-06T06:09:17.956529Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"BS = 16","metadata":{"id":"L6B922tYlMWV","execution":{"iopub.status.busy":"2022-09-06T06:09:17.959465Z","iopub.execute_input":"2022-09-06T06:09:17.960758Z","iopub.status.idle":"2022-09-06T06:09:17.971520Z","shell.execute_reply.started":"2022-09-06T06:09:17.960721Z","shell.execute_reply":"2022-09-06T06:09:17.970477Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"id":"CNBCY795lMWV","outputId":"e835b5a3-aa91-431e-889b-f1bac2d356f6","execution":{"iopub.status.busy":"2022-09-06T06:09:17.973338Z","iopub.execute_input":"2022-09-06T06:09:17.974094Z","iopub.status.idle":"2022-09-06T06:09:18.055886Z","shell.execute_reply.started":"2022-09-06T06:09:17.974006Z","shell.execute_reply":"2022-09-06T06:09:18.054770Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_head_features = torch.Tensor(X_head_train)\ntrain_body_features = torch.Tensor(X_body_train)\ntrain_targets = torch.Tensor(y_train).type(torch.LongTensor)\nval_head_features = torch.Tensor(X_head_val)\nval_body_features = torch.Tensor(X_body_val)\nval_targets = torch.Tensor(y_val).type(torch.LongTensor)\n\n\ntrainDataset = TensorDataset(train_head_features, train_body_features, train_targets)\nvalDataset = TensorDataset(val_head_features, val_body_features, val_targets)","metadata":{"id":"pKbaDLEelMWW","execution":{"iopub.status.busy":"2022-09-06T06:09:18.060492Z","iopub.execute_input":"2022-09-06T06:09:18.062695Z","iopub.status.idle":"2022-09-06T06:09:20.234582Z","shell.execute_reply.started":"2022-09-06T06:09:18.062657Z","shell.execute_reply":"2022-09-06T06:09:20.233557Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class LitLSTM(pl.LightningModule):\n    def __init__(self,num_classes,dimension,hidd_dim):\n        super(LitLSTM, self).__init__()\n        \n        self.hidd_dim = hidd_dim\n        self.n_mha = 2\n        self.embed_dim = hidd_dim\n        self.mha = nn.MultiheadAttention(self.embed_dim,self.n_mha,dropout=0.4,batch_first=True,kdim=self.hidd_dim,vdim=hidd_dim)\n        self.drop = nn.Dropout(p=0)\n        self.fc = nn.Linear(self.embed_dim, 3)\n        self.act = nn.Softmax()\n        \n        self.LSTM = nn.LSTM(input_size=dimension,\n                            hidden_size=self.hidd_dim,\n                            num_layers=2,\n                            batch_first=True,\n                            bidirectional=True)\n        \n        self.model = nn.Sequential( nn.Dropout(p=0),\n                                    nn.ReLU(inplace=True),\n                                    #nn.BatchNorm1d(self.embed_dim),\n                                    nn.Linear(self.hidd_dim, self.hidd_dim*2),\n                                    nn.Dropout(p=0),\n                                    nn.ReLU(inplace=True),\n                                    #nn.BatchNorm1d(self.embed_dim),\n                                    nn.Linear(self.hidd_dim*2, num_classes))\n        \n        # add metrics\n        self.train_acc = torchmetrics.Accuracy()\n        self.train_f1 = torchmetrics.F1Score(number_classes=num_classes,average=\"micro\")\n        self.val_acc = torchmetrics.Accuracy()\n        self.val_f1 = torchmetrics.F1Score(number_classes=num_classes,average=\"micro\")\n        \n    def forward(self, head, body):\n        h_0_head = Variable(torch.rand(4, BS, self.hidd_dim)).to(device)\n        c_0_head = Variable(torch.rand(4, BS, self.hidd_dim)).to(device)\n        h_0_body = Variable(torch.rand(4, BS, self.hidd_dim)).to(device)\n        c_0_body = Variable(torch.rand(4, BS, self.hidd_dim)).to(device)\n        head, (final_hidden_state_head, final_cell_state_head) = self.LSTM(head, (h_0_head, c_0_head))\n        body, (final_hidden_state_body, final_cell_state_body) = self.LSTM(body, (h_0_body, c_0_body))\n        h = final_hidden_state_head[-1].reshape(BS,1,-1)\n        b = final_hidden_state_body[-1].reshape(BS,1,-1)\n        mha, mha_wgts = self.mha(b,b,h)\n        x = self.model(mha).reshape(-1,3)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        head, body, labels = batch\n        # Forward pass\n        outputs = self.forward(head, body)\n        lossfn = nn.NLLLoss()\n        loss = lossfn(outputs, labels)\n        \n        y_pred = torch.exp(outputs)\n        #y_pred = output.data.max(1, keepdim=True)[1]\n        acc = self.train_acc(y_pred, labels)\n        f1 = self.train_f1(y_pred, labels)\n        # just accumulate\n\n        self.log(\"train_loss\", loss)\n        self.log(\"train_accuracy\", acc)\n        self.log(\"train_f1\", f1)\n        tensorboard_logs = {'train_loss': loss}\n        # use key 'log'\n        return {\"loss\": loss, 'log': tensorboard_logs}\n\n    # define what happens for testing here\n\n    def train_dataloader(self):\n        trainDataLoader = DataLoader(trainDataset, num_workers=2,batch_size=BS, shuffle=True,drop_last=True)\n\n        return trainDataLoader\n\n    def val_dataloader(self):\n        valDataLoader = DataLoader(valDataset, num_workers=2,batch_size=BS,shuffle=False,drop_last=True)\n        \n        return valDataLoader\n    \n    def validation_step(self, batch, batch_idx):\n        head, body, labels = batch\n        # Forward pass\n        outputs = self.forward(head, body)\n        lossfn = nn.NLLLoss()\n        loss = lossfn(outputs, labels)\n        \n        pred = torch.exp(outputs)\n        #pred = output.data.max(1, keepdim=True)[1]\n        self.val_acc.update(pred, labels)\n        self.val_f1.update(pred, labels)\n\n        self.log(\"val_loss\", loss)\n        return {\"val_loss\": loss}\n    \n    def training_epoch_end(self, training_step_outputs):\n        # compute metrics\n        train_accuracy = self.train_acc.compute()\n        train_f1 = self.train_f1.compute()\n        # log metrics\n        self.log(\"epoch_train_accuracy\", train_accuracy)\n        self.log(\"epoch_train_f1\", train_f1)\n        # reset all metrics\n        self.train_acc.reset()\n        self.train_f1.reset()\n        print(f\"\\ntraining accuracy: {train_accuracy:.4}, \"\\\n        f\"f1: {train_f1:.4}\")\n        \n    def validation_epoch_end(self, outputs):\n        # outputs = list of dictionaries\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        val_accuracy = self.val_acc.compute()\n        val_f1 = self.val_f1.compute()\n        # log metrics\n        self.log(\"val_accuracy\", val_accuracy)\n        self.log(\"val_loss\", avg_loss)\n        self.log(\"val_f1\", val_f1)\n        # reset all metrics\n        self.val_acc.reset()\n        self.val_f1.reset()\n        print(f\"\\nvalidation accuracy: {val_accuracy:.4} \"\\\n        f\"f1: {val_f1:.4}\")\n        \n        tensorboard_logs = {'avg_val_loss': avg_loss}\n        # use key 'log'\n        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n    \n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters())","metadata":{"id":"3IYNfqwmlMWW","execution":{"iopub.status.busy":"2022-09-06T06:09:20.236521Z","iopub.execute_input":"2022-09-06T06:09:20.236895Z","iopub.status.idle":"2022-09-06T06:09:20.262210Z","shell.execute_reply.started":"2022-09-06T06:09:20.236859Z","shell.execute_reply":"2022-09-06T06:09:20.260978Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# seeding for torch, numpy, stdlib random, including DataLoader workers!\n#seed_everything(123, workers=True)\n\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\",\n    stopping_threshold=1e-6,\n    divergence_threshold=9.0,\n    check_finite=True)","metadata":{"id":"L4hEqrZNlMWY","outputId":"18fb83dd-6e01-444f-e663-eac86743db41","execution":{"iopub.status.busy":"2022-09-06T06:13:14.685345Z","iopub.execute_input":"2022-09-06T06:13:14.687241Z","iopub.status.idle":"2022-09-06T06:13:14.693959Z","shell.execute_reply.started":"2022-09-06T06:13:14.687195Z","shell.execute_reply":"2022-09-06T06:13:14.692191Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"%reload_ext tensorboard\nmodel = LitLSTM(dimension=X_head_train.shape[2],hidd_dim=100,num_classes=3)\ntrainer = Trainer(accelerator='gpu',devices=1,max_epochs=100,log_every_n_steps=8)#,callbacks=[early_stopping])\ntrainer.fit(model)","metadata":{"id":"t4_MQMv8lMWZ","outputId":"9ddf322b-f121-49f0-d52c-2aab40546911","execution":{"iopub.status.busy":"2022-09-06T06:13:15.637556Z","iopub.execute_input":"2022-09-06T06:13:15.637957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tensorboard --logdir lightning_logs","metadata":{"id":"sAT-RzxRlMWZ","outputId":"a48eada9-2d0b-4ebe-f3d1-6f60b9e73f3a","execution":{"iopub.status.busy":"2022-09-06T06:13:08.797514Z","iopub.execute_input":"2022-09-06T06:13:08.798717Z","iopub.status.idle":"2022-09-06T06:13:14.682332Z","shell.execute_reply.started":"2022-09-06T06:13:08.798659Z","shell.execute_reply":"2022-09-06T06:13:14.680983Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
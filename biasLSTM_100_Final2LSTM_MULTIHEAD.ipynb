{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student1/sushovan.saha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "import W2V3\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word in vocabulary 21678\n",
      "word in sentences are replaced with word ID [[    0     0     0 ...    83  3352   403]\n",
      " [    0     0     0 ...   244  5664    30]\n",
      " [    0     0     0 ...  4939    70  3962]\n",
      " ...\n",
      " [    0     0     0 ...   420  8176 21678]\n",
      " [    0     0     0 ...   397   264  1499]\n",
      " [    0     0     0 ...    47   435     1]]\n",
      "The size of vocabulary  21679\n",
      "Loaded 400000 word vectors.\n",
      "(26590, 120, 100)\n",
      "number of word in vocabulary 143699\n",
      "word in sentences are replaced with word ID [[   3    2   24 ...    9  114 1996]\n",
      " [9796  590 2684 ...   61 1065 3100]\n",
      " [1958 2002 7091 ... 7257 2133 1951]\n",
      " ...\n",
      " [   2  900   94 ...  156  882   19]\n",
      " [   2   58 1794 ...   57   47   84]\n",
      " [3262 3233 4319 ...  472  382 2286]]\n",
      "The size of vocabulary  143700\n",
      "Loaded 400000 word vectors.\n",
      "(26590, 120, 100)\n"
     ]
    }
   ],
   "source": [
    "X_head_train,X_body_train,y_train = W2V3.Sentence2Vec(filename='Train.xlsx',glovepath=glove_path, embedding_dim=100, max_length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word in vocabulary 4343\n",
      "word in sentences are replaced with word ID [[   0    0    0 ...    7 1623  645]\n",
      " [   0    0    0 ...   87 1626  646]\n",
      " [   0    0    0 ...  179  202  647]\n",
      " ...\n",
      " [   0    0    0 ...  147 4339   29]\n",
      " [   0    0    0 ... 4342 4343 1261]\n",
      " [   0    0    0 ...  125   22 1619]]\n",
      "The size of vocabulary  4344\n",
      "Loaded 400000 word vectors.\n",
      "(1300, 120, 100)\n",
      "number of word in vocabulary 37490\n",
      "word in sentences are replaced with word ID [[   15  2444  1170 ...  6866  6867   292]\n",
      " [ 1849  1254 17674 ... 10294   477   271]\n",
      " [   66  3773   922 ...   352  1242    17]\n",
      " ...\n",
      " [  252   610  3654 ...  2282  1443   403]\n",
      " [   36    49    73 ...  4708   219   997]\n",
      " [  452   312   956 ...    12   394     6]]\n",
      "The size of vocabulary  37491\n",
      "Loaded 400000 word vectors.\n",
      "(1300, 120, 100)\n"
     ]
    }
   ],
   "source": [
    "X_head_test,X_body_test,y_test = W2V3.Sentence2Vec(filename='Test.xlsx',glovepath=glove_path, embedding_dim=100, max_length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300, 120, 100)\n",
      "(1300, 120, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_head_test.shape)\n",
    "print(X_body_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_body_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    10241\n",
       "0     8861\n",
       "1     7488\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    599\n",
       "0    402\n",
       "1    299\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = X_body_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class myLSTM(nn.Module):\n",
    "\n",
    " \n",
    "\n",
    "    def __init__(self, dimension, hidd_dim):\n",
    "        super().__init__()\n",
    "        self.hidd_dim = hidd_dim\n",
    "        self.n_mha = 2\n",
    "        self.embed_dim = hidd_dim\n",
    "        self.lstm = nn.LSTM(input_size=dimension,\n",
    "                            hidden_size=self.hidd_dim,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.mha = nn.MultiheadAttention(self.embed_dim,self.n_mha,dropout=0,batch_first=True,kdim=self.hidd_dim,vdim=hidd_dim)\n",
    "        self.fc = nn.Linear(self.embed_dim, 3)\n",
    "        self.act = nn.Softmax()\n",
    "\n",
    " \n",
    "\n",
    "    def forward(self, head, body, batch_size):\n",
    "        np.random.seed(1)\n",
    "        h_0_head = Variable(torch.rand(2, batch_size, self.hidd_dim))\n",
    "        c_0_head = Variable(torch.rand(2, batch_size, self.hidd_dim))\n",
    "        h_0_body = Variable(torch.rand(2, batch_size, self.hidd_dim))\n",
    "        c_0_body = Variable(torch.rand(2, batch_size, self.hidd_dim))\n",
    "        \n",
    "        head, (final_hidden_state_head, final_cell_state_head) = self.lstm(head, (h_0_head, c_0_head))\n",
    "        body, (final_hidden_state_body, final_cell_state_body) = self.lstm(body, (h_0_body, c_0_body))\n",
    "        \n",
    "        h = final_hidden_state_head[-1]\n",
    "        b = final_hidden_state_body[-1]\n",
    "        mha, mha_wgts = self.mha(h,b,b)\n",
    "        x = self.fc(mha)\n",
    "        #x = self.act(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, head,body, y, loss_fn, optimizer, batch_size=1024):\n",
    "    model.train()\n",
    "    head,body, y = torch.Tensor(head),torch.Tensor(head), torch.Tensor(y)\n",
    "    y = y.type(torch.LongTensor)\n",
    "    net_loss = 0\n",
    "    \n",
    "    for i in range(0, len(head), batch_size):\n",
    "        i_end = i+batch_size\n",
    "        head_batch = head[i:min(i_end, len(head))]\n",
    "        body_batch = body[i:min(i_end, len(body))]\n",
    "        y_batch = y[i:min(i_end, len(head))]\n",
    "        \n",
    "        pred = model(head_batch,body_batch,head_batch.shape[0])\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        net_loss = net_loss+loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return net_loss\n",
    "\n",
    "def test(model, head,body, yts,batchsize):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        head = torch.Tensor(head)\n",
    "        body = torch.Tensor(body)\n",
    "        pred = model(head,body,batchsize)\n",
    "        loss = loss_fn(pred, torch.Tensor(yts).type(torch.LongTensor))\n",
    "        yhat = np.argmax(pred, axis=1).numpy()\n",
    "        acc = np.sum(yhat==yts)*100/yhat.shape[0]\n",
    "        f1 = metrics.f1_score(yts,yhat,average=None)\n",
    "        return acc, f1,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 ---\n",
      "\tCross Entropy Loss (Training) : 113.47804319858551 \n",
      "\tTrain Accuracy : 38.51 % \n",
      "\tTest Accuracy : 46.08 % \n",
      "--- Epoch 2 ---\n",
      "\tCross Entropy Loss (Training) : 113.44297754764557 \n"
     ]
    }
   ],
   "source": [
    "model = myLSTM(X_body_train.shape[2], hidd_dim=100)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "batch_size = 256\n",
    "epochs = 70\n",
    "train_loss = []\n",
    "test_acc = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"--- Epoch {} ---\".format(i+1))\n",
    "    epoch_loss = train(model,X_head_train, X_body_train, y_train, loss_fn, optimizer, batch_size)\n",
    "    train_loss.append(epoch_loss)\n",
    "    print(\"\\tCross Entropy Loss (Training) : {} \".format(epoch_loss))\n",
    "\n",
    "    acc, _, __ = test(model, X_head_train, X_body_train, y_train, batchsize=X_body_train.shape[0])\n",
    "    print(\"\\tTrain Accuracy : {:.2f} % \".format(acc))\n",
    "    train_acc.append(acc)\n",
    "    \n",
    "    acc, _, tst_loss = test(model, X_head_test, X_body_test, y_test, batchsize=X_body_test.shape[0])\n",
    "    print(\"\\tTest Accuracy : {:.2f} % \".format(acc))\n",
    "    test_acc.append(acc)\n",
    "    test_loss.append(tst_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(epochs), train_loss, label=\"train loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(epochs), test_loss, label=\"test loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(epochs), train_acc, label=\"train accuracy\")\n",
    "plt.plot(range(epochs), test_acc, label=\"test accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1, _ = test(model, X_head_test,X_body_test, y_test, batchsize=X_body_test.shape[0])\n",
    "print(\"Test Accuracy : {:.2f} % \".format(acc))\n",
    "print(\"Test Class-wise F1 Score : \\n{}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3cbbd3491d2f6c98b8c3d8ad5891315a0953a3882f43d17a58845e943883b72"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
